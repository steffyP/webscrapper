# -*- coding: utf-8 -*-
"""
    Simple scraper for downloading videos and files from one pentesteracademy module

    Note: this requires having an active pentesteracademy account
    You will need to provide a valid cookie to make this work :)
    The TODOs mark the places where you will have to adapt the script for your needs

    The intention of this scrapper to download all files related to one module
    for convenience, e.g. if you want to access the files offline because you
    are travelling.

    For details on scrapy see the documentation https://scrapy.readthedocs.io

    start this with 'scrapy crawl examples'


"""

import scrapy


class ExampleSpider(scrapy.Spider):

    save_to = 'INSERT_DIR_HERE' # TODO replace with actual dir
    name = 'example'
    user_agent = 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:54.0) Gecko/20100101 Firefox/54.0' # TODO you can adapt this if you wish
    cookie = 'INSERT_COOKIE_HERE' # TODO needs to be replaced with your cookie
    def start_requests(self):
        #print self.cookie
        urls = [
            # TODO replace with required url
            'http://www.pentesteracademy.com/course?id=10#profile',

        ]
        for url in urls:
             request = scrapy.Request(url=url, callback=self.parse, cookies={'ACSID':self.cookie},errback=self.handle_error)
             request.headers['User-Agent'] = self.user_agent

             yield request

    def create_name(self, num, name, text):
        """
            creates name for the downloaded file,
            - starting with a zero-padded 3-digit ennumberation
            - including the filename
            - and the file's extension
        """
        file_name = str(num).zfill(3)
        text = text.strip()
        text = text.lstrip()
        text = text.rstrip()
        file_name = file_name + "_" + str(name) + "_" + text
        if text == "Video":
            file_name = file_name + ".mp4"
        if "pdf" in text or "Pdf" in text or "PDF" in text:
            file_name = file_name + ".pdf"
        file_name = file_name.replace("/", "-")
        file_name = file_name.replace(":", "-")
        file_name = file_name.replace(" ", "-")
        file_name = file_name.replace("?", "")
        file_name = file_name.replace("!", "")
        file_name = file_name.replace("(", "")
        file_name = file_name.replace(")", "")
        file_name = file_name.replace(";", "")
        file_name = file_name.replace("--", "-")
        file_name = file_name.replace("\r\n", "")
        return file_name

    def parse_second(self, response):
        """
            Searches the "details" site for links, and downloads the files
        """

        print 'trying to crawl number ' + str(response.meta['number'])
        #if response.meta['number'] in (15, 26, 27, 34):
        columns = response.xpath("//div[@id='home']//a")
        for column in columns:
                text = column.xpath("text()").extract_first()
                link = column.xpath("@href").extract_first()
                if link not in "http":
                    link = response.urljoin(link)

                print "found " + text + "(" + link + ")"

                file_name = self.create_name(response.meta['number'], response.meta['name'], text)
                #print "created filename " + file_name + " and link: " + str(link)

                request = scrapy.Request(link, callback=self.save_file,cookies={'ACSID':self.cookie},errback=self.handle_error)
                request.headers['User-Agent'] = self.user_agent

                request.meta['name'] = file_name
                yield request

    def save_file(self, response):
        """
            This is the actual download task, stores the file to the
            given directory
        """
        
        file_name = response.meta['name']

        path = self.save_to + "/" + file_name
        with open(path, 'wb') as f:
            f.write(response.body)
        print "saved file " + file_name

        #print response
    def handle_error(self, failure):
        self.log("Request failed: %s" % failure.request)

    def parse(self, response):
            """
                Starts the parsing, for each module in the course, the link will be
                followed to the details page, where the actual
            """

            print "started to run scrappy"
            number = 1

        #    print str(response.xpath("count(//a[contains(@href, 'video')])"))

            selected = response.xpath("//h4[@class='media-heading']/a")
            #print str(selected)
            for a in selected:
                #print a
            #div = response.css("div.media div.media-body h4 a")[0]
            #print "found div" + str(div)
                name = a.css("::text").extract_first()
                link = a.css("::attr(href)").extract_first()
                print name
                if link is not None:
                    link = response.urljoin(link)
                    print str(number) + " following link: " + str(link)
                    request = scrapy.Request(link, callback=self.parse_second,cookies={'ACSID':self.cookie},errback=self.handle_error)
                    request.headers['User-Agent'] = self.user_agent
                    request.meta['number'] = number
                    request.meta['name'] = name
                    number = number + 1
                    yield request
